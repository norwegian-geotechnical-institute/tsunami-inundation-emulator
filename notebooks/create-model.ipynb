{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NCDatasets, Flux, CUDA, Plots, DelimitedFiles, Distributed, Dates, Measures\n",
    "using Dates: format, now, value, Millisecond\n",
    "import YAML\n",
    "using BSON: @save, @load\n",
    "\n",
    "gr(fmt =:png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory to project base.\n",
    "cd(\"..\")\n",
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cuda functionality\n",
    "CUDA.functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used as a first step towards the construction of a model. The use of a notebook makes it much easier to select data and construct a model. In this notebook we select the input and output, defines the model, create a config file. All of this is written to file in a new run-directory. \n",
    "\n",
    "To train the model we copy the script `train-model.jl` to the new run folder and runs the current version from that folder (loading config, model and data selection files).\n",
    "\n",
    "\n",
    "# Selecting input and output.\n",
    "The dataset is of relatively high resolution, hence it might be a good idea to reduce the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/data_large/stg/UMA_download/\"; # Path to the folder with the scenario files.\n",
    "train_data = \"article_data/train_591/train.txt\"; # Text file with a list of scenario files in the data_dir.\n",
    "test_data = \"article_data/bottom_UMAPS_shuf.txt\";\n",
    "grid_file = \"/data_large/grids/Catania/C_CT.grd\"; # Topography file at the inundation location.\n",
    "runs_dir = \"runs/\"; # folder to store run within subfolder.\n",
    "train_script =\"src/train-model.jl\";\n",
    "ct_mask_file = \"article_data/ct_mask.txt\"; # Pixels target of predictions (true/false).\n",
    "reg = 1e7/(591^3); # Parameter for l2 weight penalization in the loss function.\n",
    " \n",
    "batch_size = 30;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting inundation area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_slice = (1:912,1:2224) # Subwindow of the inundation.\n",
    "scale = (1,1) # Downsample dimensions - window of size scale is mapped to single pixel. Make sure you get integer dimension.\n",
    "dims = (length(ct_slice[1]) รท scale[1],length(ct_slice[2]) รท scale[2]) # dimension of inundation map (matrix).\n",
    "aspect_ratio = scale[1]/scale[2] # Pixel shape.\n",
    "\n",
    "downsampler = MaxPool(scale)\n",
    "upsampler = Upsample(scale)\n",
    "\n",
    "topography = NCDataset(grid_file, \"r\")[\"z\"][ct_slice...];\n",
    "\n",
    "function downsample(x, scale)\n",
    "    x = Array{Float32}(reshape(x, (size(topography)...,1,1)))\n",
    "    return MaxPool(scale)(x)[:,:,1,1]\n",
    "end\n",
    "\n",
    "downsampled_topography = downsample(topography, scale);\n",
    "ct_mask = downsample(BitArray(readdlm(ct_mask_file, '\\t',Bool, '\\n'))[ct_slice...], scale) |> BitArray;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(downsampled_topography) == dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maps may now be truncated according to the mask.\n",
    "topo_truncated = zeros(Float32, dims);\n",
    "topo_truncated[ct_mask] = downsampled_topography[ct_mask];\n",
    "\n",
    "p1 = heatmap(\n",
    "    downsampled_topography'; \n",
    "    aspect_ratio=1/aspect_ratio, \n",
    "    xlim=(1,dims[1]), \n",
    "    ylim=(1,dims[2]),\n",
    "    c=:oleron, \n",
    "    clims = (-30,30),\n",
    "    margins= 3mm\n",
    ")\n",
    "p2 = heatmap(\n",
    "    topo_truncated'; \n",
    "    aspect_ratio=1/aspect_ratio, \n",
    "    xlim=(1,dims[1]), \n",
    "    ylim=(1,dims[2]),\n",
    "    c=:oleron, \n",
    "    clims = (-30,30),\n",
    "    margins= 3mm\n",
    ")\n",
    "\n",
    "p3 = heatmap(\n",
    "    ct_mask'; \n",
    "    aspect_ratio=1/aspect_ratio, \n",
    "    xlim=(1,dims[1]), \n",
    "    ylim=(1,dims[2]),\n",
    "    margins = 3mm\n",
    ")\n",
    "\n",
    "plot(\n",
    "    p1, p2, p3,\n",
    "    layout = (1,3), \n",
    "    size=(1000,600), \n",
    "    title=[\"Topography\" \"Truncated Topography\" \"Mask\"], \n",
    "    titleloc= :center\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(ct_mask) # Number of pixels for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries data\n",
    "\n",
    "Inspection of timeseries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting arbitrary scenario file for inspection.\n",
    "scenario_ts = \"10_PS_2803/2054_E02020N3739E02658N3366-PS-Str_PYes_Hom-M888_E02122N3652_S004_ts.nc\"\n",
    "isfile(joinpath(data_dir, scenario_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinpath(data_dir, scenario_ts) # Complete path to the scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_ts = NCDataset(joinpath(data_dir, scenario_ts), \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_slice = 30:45,1:480 # gauge number, time\n",
    "data_ts[\"eta\"][ts_slice...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the \n",
    "t_0 = data_ts[\"time\"][ts_slice[2][1]]\n",
    "time_scale = [round((t-t_0)/Millisecond(1000*60)) for t in data_ts[\"time\"][ts_slice[2]]]\n",
    "p = heatmap(data_ts[\"eta\"][ts_slice...], \n",
    "    c=:deepsea, \n",
    "    legend=:none, \n",
    "    xlabel=\"Time [Minutes]\", \n",
    "    xticks=(ts_slice[2][1:40:480], time_scale[1:40:480]), \n",
    "    xrotation = 45,\n",
    "    ylabel=\"POI's\", \n",
    "    yticks=(Array(1:2:16), Array(30:2:46))\n",
    ")\n",
    "display(p)\n",
    "#savefig(\"timeseries_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "Create datareader first, in order to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add process for dataloading.\n",
    "addprocs(1; exeflags=\"--project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere include(\"scripts/datareader.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Dict(\n",
    "    \"data_dir\" => data_dir,\n",
    "    \"train_data\" => train_data,\n",
    "    \"test_data\" => test_data,\n",
    "    \"grid_file\" => grid_file,\n",
    "    \"batch_size\" => batch_size,\n",
    "    \"ct_slice\" => ct_slice,\n",
    "    \"ts_slice\" => ts_slice,\n",
    "    \"scale\" => scale, \n",
    "    \"reg\" => reg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere begin\n",
    "    reader = DataReader.Reader($config)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = DataReader.scenarios(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in 1:100\n",
    "    scenario = take!(scenarios)\n",
    "    @info \"Scenario $i is $scenario\"\n",
    "end\n",
    "\n",
    "@info \"Load batches with scenarios.\"\n",
    "batches = RemoteChannel(()->Channel(4))\n",
    "\n",
    "for worker in workers()\n",
    "    remote_do(reader, worker, scenarios, batches)\n",
    "end\n",
    "\n",
    "batch = take!(batches);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.scenario_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.flow_depths |> size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.deformed_topographies |> size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.etas |> size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = heatmap(\n",
    "    batch.flow_depths[:,:,1,1]', \n",
    "    aspect_ratio=1/aspect_ratio,\n",
    "    xlim=(1,dims[1]), \n",
    "    ylim=(1,dims[2])\n",
    ")\n",
    "p2 = heatmap(\n",
    "    batch.deformed_topographies[:,:,1,1]', \n",
    "    aspect_ratio=1/aspect_ratio,\n",
    "    c=:oleron, \n",
    "    clims = (-30,30),\n",
    "    xlim=(1,dims[1]), \n",
    "    ylim=(1,dims[2])\n",
    ")\n",
    "\n",
    "plot(\n",
    "    p1, p2, \n",
    "    layout = (1,2), \n",
    "    size=(1200,700), \n",
    "    title=[\"Flow Depth\" \"Deformed Topography\"], \n",
    "    titleloc= :center\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model in a separate file.\n",
    "\n",
    "There are issues with storing more complex models using BSON.jl. Teherfore we use a library to specifically write weights to file. This means we have to define model structure in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"scripts/model_config.jl\")\n",
    "\n",
    "model = ModelConfig.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mc32_l16_rel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ADAM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create rundir with appropriate config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function make_run_dir()\n",
    "    timestamp = format(now(), \"YYYYmmdd-HHMMSS\")\n",
    "    #dir_name = joinpath(runs_dir, model_name*\"_$timestamp\")\n",
    "    dir_name = joinpath(runs_dir, model_name)\n",
    "    @assert !ispath(dir_name) \"Output directory already exists\"\n",
    "    mkpath(dir_name)\n",
    "    return dir_name\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rundir = make_run_dir()\n",
    "#rundir = \"article_runs/stridedConv_20230308-174706/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write mask to file.\n",
    "open(joinpath(rundir, \"ct_mask.txt\"), \"w\") do io\n",
    "           writedlm(io, ct_mask)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Dict(\n",
    "    \"data_dir\" => data_dir,\n",
    "    \"train_data\" => joinpath(pwd(), train_data),\n",
    "    \"test_data\" => joinpath(pwd(), test_data),\n",
    "    \"grid_file\" => grid_file,\n",
    "    \"model_name\" => model_name,\n",
    "    \"rundir\" => joinpath(pwd(), rundir),\n",
    "    \"batch_size\" => batch_size,\n",
    "    \"ct_slice\" => ct_slice,\n",
    "    \"scale\" => scale,\n",
    "    \"dims\" => dims,\n",
    "    \"ts_slice\" => ts_slice,\n",
    "    \"max_batch_nr\" => 20000,\n",
    "    \"gpu\" => true,\n",
    "    \"reg\" => reg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML.write_file(joinpath(rundir, \"config.yml\"), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@save joinpath(rundir, \"optimizer.bson\") opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelConfig.save(model, joinpath(rundir,\"$(model_name).jls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy scripts for training to rundir. Next run the script \n",
    "cp(\"scripts/train-model.jl\", joinpath(rundir,\"train-model.jl\"), force=true)\n",
    "cp(\"scripts/datareader.jl\", joinpath(rundir, \"datareader.jl\"), force=true)\n",
    "cp(\"scripts/model_config.jl\", joinpath(rundir, \"model_config.jl\"), force=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to run the training, run the script from the new run directory\n",
    "```terminal\n",
    "[rundir]$ julia --project train-model.jl\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
